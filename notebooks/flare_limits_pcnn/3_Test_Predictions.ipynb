{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Predictions\n",
    "\n",
    "Notebook to compute and store predictions on the operational test set.\n",
    "\n",
    "Full-disk and patches/sector predictions are stored in separated files "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "\n",
    "if COLAB : \n",
    "  configSetup = {\n",
    "      'COLAB'           : 'True',\n",
    "      'PATH_ROOT_DRIVE' : '/content/drive/MyDrive/Projects/Forecast',\n",
    "      'PATH_ROOT_LOCAL' : '/content/session',\n",
    "      'PATH_SUNDL'      : '/content/sundl',\n",
    "      'PATH_PROJECT'    : '/content/sundl/notebooks/flare_limits_pcnn'\n",
    "  }\n",
    "  !git clone https://github.com/gfrancisco20/sundl.git\n",
    "  import sys\n",
    "  import re\n",
    "  sys.path.append(configSetup['PATH_SUNDL'])\n",
    "  sys.path.append(configSetup['PATH_PROJECT'])\n",
    "  configFile = f'{configSetup[\"PATH_PROJECT\"]}/config.py'\n",
    "  with open(configFile, 'r') as file:\n",
    "    content = file.read()\n",
    "  for constant in configSetup.keys():\n",
    "    content = re.sub(re.compile(f'{constant} = .*'), f'{constant} = \\'{configSetup[constant]}\\'', content)\n",
    "  with open(configFile, 'w') as file:\n",
    "    file.write(content)\n",
    "   \n",
    "from config import *\n",
    "from sundl.utils.colab import mountDrive\n",
    "if COLAB:\n",
    "  # mouting drive content in session on colab\n",
    "  mountDrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER = PATH_RES/'Results_Paper_PCNN' #\n",
    "\n",
    "# Seleted Models from the CV resullts\n",
    "modelDict = {\n",
    "  'C+_mpf_Persistant_24'                                    : 'C+_Persistant',\n",
    "  'C+_mpf_PTx8_RtdXall_ProgPos_AW4e6D4e3_blos_24'           : 'C+_SPCNN_Blos', #\n",
    "  'C+_mpf_PTx8_RtdXall_ProgPos_AW1e5D1e4_0193x0211x0094_24' : 'C+_SPCNN_EUV',\n",
    "  'M+_mpf_Persistant_24'                                    : 'M+_Persistant',\n",
    "  'M+_mpf_PTx8_RtdXall_LowC_AW6e6D1e3_blos_24'              : 'M+_SPCNN_Blos',\n",
    "  'M+_mpf_PTx8_RtdXall_ProgPos_AW1e5D1e4_0193x0211x0094_24' : 'M+_SPCNN_EUV',\n",
    "}\n",
    "modelDictRev = {modelDict[oldName] : oldName for oldName in modelDict.keys()}\n",
    "\n",
    "ensembles = {'C+_SPCNN_Both_Max'  : ['C+_SPCNN_Blos','C+_SPCNN_EUV'],\n",
    "             'M+_SPCNN_Both_Max'  : ['M+_SPCNN_Blos','M+_SPCNN_EUV'], \n",
    "             'C+_SPCNN_Both_Avg'  : ['C+_SPCNN_Blos','C+_SPCNN_EUV'],\n",
    "             'M+_SPCNN_Both_Avg'  : ['M+_SPCNN_Blos','M+_SPCNN_EUV'] \n",
    "            #  'C+_SPCNN_Histo' : ['C+_SPCNN_Blos','C+_SPCNN_EUV','C+_Persistant'],\n",
    "            #  'M+_SPCNN_Histo' : ['M+_SPCNN_Blos','M+_SPCNN_EUV', 'M+_Persistant']\n",
    "             }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import dill as pickle\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from sundl.utils.data import read_Dataframe_With_Dates\n",
    "\n",
    "for modelName in modelDict.keys():\n",
    "  # TBR\n",
    "  if modelName.split('_')[2][3] in ['4']:\n",
    "    # TODO : make generic to patch number.\n",
    "    continue\n",
    "  \n",
    "  labelCol = modelName.split('_')[1]\n",
    "  h = modelName.split('_')[-1]\n",
    "\n",
    "  pathPredFd = F_PATH_PREDS(FOLDER)/f'{modelName}_fd.csv'\n",
    "  pathPredPt = F_PATH_PREDS(FOLDER)/f'{modelName}_pt.csv'\n",
    "  if not pathPredFd.exists():\n",
    "    print('\\n\\nMODEL : ',modelName)\n",
    "\n",
    "    # loading model path and configs for availlable fold models\n",
    "    modelsConfigAndPath = glob((FOLDER/f'models/{modelName}*').as_posix())\n",
    "    modelFoldsPath = [m for m in modelsConfigAndPath if m[-3:]!='pkl']\n",
    "    modelFoldsConfigs = [m for m in modelsConfigAndPath if m[-3:]=='pkl']\n",
    "    foldIds = [m[-5:] for m in modelFoldsPath]\n",
    "    n_fold = len(foldIds)\n",
    "    if n_fold == 0:\n",
    "      continue\n",
    "    for foldIdx in range(n_fold-1,-1,-1):\n",
    "      if not Path(f'{modelFoldsPath[foldIdx]}/assets').exists():\n",
    "        modelFoldsPath.pop(foldIdx)\n",
    "        modelFoldsConfigs.pop(foldIdx)\n",
    "        foldIds.pop(foldIdx)\n",
    "    print('FOLDS WITH MODELS: ',foldIds)\n",
    "    if len(foldIds)==0:\n",
    "      continue\n",
    "    with open(modelFoldsConfigs[0], 'rb') as f1:\n",
    "      config = pickle.load(f1)\n",
    "    dfTest = read_Dataframe_With_Dates(F_PATH_TEST(configTest['labelCol'],configTest['ts_off_label_hours']))\n",
    "    configTest = config['dataset_val']\n",
    "    configTest['dfId2History'] = dfTest.copy()\n",
    "    configTest['samples'] = None\n",
    "    configTest['cache'] = False\n",
    "    configTest['shuffle'] = False\n",
    "    configTest['weightByClass'] = False\n",
    "    configTest['batch_size'] = 64\n",
    "    configTest['epochs'] = 1\n",
    "    # STOP\n",
    "    # encoders consstantss **\n",
    "    classTresholds = configTest['classTresholds']\n",
    "    binCls = modelName[0]\n",
    "    # **\n",
    "    if modelName.split('_')[2] == 'Persistant':\n",
    "      # dsTest , _, _, _ = build_dataset_persistant(**configTest)\n",
    "      # saved pstmod not working\n",
    "      dfId2History =  dfTest.copy()\n",
    "      ts_off_label_hours = h\n",
    "      ts_off_history_hours = -h\n",
    "      dfId2History.index = dfId2History.index.shift(periods = -ts_off_label_hours, freq='H')\n",
    "      input_lag = - ts_off_history_hours\n",
    "      dfId2History['history'] = dfId2History[labelCol].rolling(window = f'{input_lag}H',\n",
    "                                                      closed = 'right', # min_periods = int(input_lag)\n",
    "                                                      ).apply(\n",
    "                                                          lambda x: x[0]) # we remove first month in case of incomplete windows\n",
    "      dfId2History = dfId2History[int(ts_off_label_hours/2):-int(ts_off_label_hours/2)]\n",
    "      dfId2History = dfId2History.copy()\n",
    "\n",
    "      dfId2History['pred'] = dfId2History['history'].apply(lambda x: configTest['labelEncoder'](x))\n",
    "      dfId2History['label'] = dfId2History[labelCol].apply(lambda x: configTest['labelEncoder'](x))\n",
    "      dfPred = dfId2History[[labelCol,'cls','label','pred']].copy()\n",
    "      dfPred.to_csv(pathPredFd)\n",
    "    else:\n",
    "      if 'buildDsFunction' in config.keys():\n",
    "        buildDs = config[f'buildDsFunction']\n",
    "      else:\n",
    "        # default dataset builder if not stored in config (adapt if needed)\n",
    "        buildDs = builDS_image_feature\n",
    "      dsTest , _, missing_file_regexp, dfSamples_corr = buildDs(**configTest)\n",
    "      dfSamples_corr = dfSamples_corr.set_index('timestamp',drop = True)\n",
    "\n",
    "      # predictionss\n",
    "      predsFd = []\n",
    "      patch_predsFd = []\n",
    "      for foldIdx,modelFoldPath in enumerate(modelFoldsPath):\n",
    "        # Full-disk predictions\n",
    "        model = tf.keras.models.load_model(modelFoldPath, compile=False)\n",
    "        predsFd.append(model.predict(dsTest))\n",
    "        # Patches predictions\n",
    "        if modelName.split('_')[2][:2] == 'PT':\n",
    "          for layer in model.layers:\n",
    "            if layer.name == 'time_distributed':\n",
    "              patchesBlock = layer\n",
    "          patches = tf.keras.Model(model.input, patchesBlock.output, name='patches')\n",
    "          patch_predsFd.append(patches.predict(dsTest))\n",
    "          # predsFd could be retrieved from it to avoid double computation\n",
    "          del patches\n",
    "        del model\n",
    "      del dsTest\n",
    "\n",
    "      # predictions to dataframes\n",
    "      dfPred = dfSamples_corr[[labelCol,'cls']].copy()\n",
    "      dfPred['label'] = dfPred[labelCol].apply(lambda x: configTest['labelEncoder'](x))\n",
    "      dfPred['pred'] = np.zeros(len(dfPred))\n",
    "\n",
    "      for idx,foldId in enumerate(foldIds):\n",
    "        dfPred[f'pred_{foldId}'] = predsFd[idx][:,1]\n",
    "        dfPred['pred'] += dfPred[f'pred_{foldId}']\n",
    "      dfPred['pred'] /= (idx+1)\n",
    "      dfPred.to_csv(pathPredFd)\n",
    "\n",
    "      if modelName.split('_')[2][:2] == 'PT':\n",
    "        dfPredPatches = dfSamples_corr[[labelCol,'cls']].copy()\n",
    "        dfPredPatches['label'] = dfPredPatches[labelCol].apply(lambda x: configTest['labelEncoder'](x))\n",
    "        num_ptch = patch_predsFd[0].shape[1]\n",
    "        for ptchId in range(num_ptch):\n",
    "          dfPredPatches[f'pred_pt{ptchId}'] = np.zeros(len(dfPredPatches))\n",
    "          for idx,foldId in enumerate(foldIds):\n",
    "            dfPredPatches[f'pred_pt{ptchId}_{foldId}'] = patch_predsFd[idx][:,ptchId,0]\n",
    "            dfPredPatches[f'pred_pt{ptchId}']  += dfPredPatches[f'pred_pt{ptchId}_{foldId}']\n",
    "          dfPredPatches[f'pred_pt{ptchId}'] /= (idx+1)\n",
    "          \n",
    "print('Ensemble models prediction')\n",
    "for modelName in ensembles.keys():\n",
    "  ensModIsMax = False\n",
    "  if modelName.split('_')[-1] in ['max','Max']:\n",
    "    ensModIsMax = True\n",
    "  pathPredFd =  F_PATH_PREDS(FOLDER)/f'{modelName}_fd.csv'\n",
    "  pathPredPt =  F_PATH_PREDS(FOLDER)/f'{modelName}_pt.csv'\n",
    "  if not pathPredPt.exists():\n",
    "    for pathPred in [pathPredFd,pathPredPt]:\n",
    "      predTag = pathPred.as_posix().split('_')[-1][:2]\n",
    "      for idx,subModelName in enumerate(ensembles[modelName]):\n",
    "        subModel = read_Dataframe_With_Dates(F_PATH_PREDS(FOLDER)/f'{modelDictRev[subModelName]}_{predTag}.csv')\n",
    "        if idx == 0:\n",
    "          dfPred = subModel.copy()\n",
    "          pred_cols = [col for col in dfPred.columns if col[:3]=='pre']\n",
    "          ptcPredCols = [col for col in dfPred.columns if len(col)==len('pred_pt0')]\n",
    "          num_ptc = len(ptcPredCols)\n",
    "        else:\n",
    "          # keeping only common dates\n",
    "          tmp = subModel.copy()\n",
    "          tmp = tmp[tmp.index.isin(dfPred.index)].sort_index()\n",
    "          dfPred = dfPred[dfPred.index.isin(tmp.index)].sort_index()\n",
    "          # averaging\n",
    "          if modelDictRev[subModelName].split('_')[2] == 'Persistant':\n",
    "            # empirical persistent probability\n",
    "            for ptchTag in [''] + [f'_pt{ptcId}' for ptcId in range(num_ptc)]:\n",
    "              c = tmp[f'change{ptchTag}'].sum()\n",
    "              tot = len(tmp)\n",
    "              pChange = c/ (2*tot)\n",
    "              tmp[tmp[f'histo{ptchTag}']==1][f'pred{ptchTag}'] = 1 - pChange # prob of positive event\n",
    "              tmp[tmp[f'histo{ptchTag}']==0][f'pred{ptchTag}'] = pChange # prob of positive event\n",
    "            for col in pred_cols:\n",
    "              if ensModIsMax:\n",
    "                dfPred[col] = np.maximum(dfPred[col],tmp[col])\n",
    "              else:\n",
    "                dfPred[col] = dfPred[col] + tmp[col]\n",
    "          else:\n",
    "            missingCols = [col for col in pred_cols if col not in tmp.columns]\n",
    "            for col in missingCols:\n",
    "              tmp[col] = dfPred[col]\n",
    "            if ensModIsMax:\n",
    "              for col in pred_cols:\n",
    "                dfPred[col] = np.maximum(dfPred[col],tmp[col])\n",
    "            else:\n",
    "              dfPred[pred_cols] = dfPred[pred_cols] + tmp[pred_cols]\n",
    "      if not ensModIsMax:\n",
    "        dfPred[pred_cols] = dfPred[pred_cols]/len(ensembles[modelName])\n",
    "      dfPred = dfPred[['mpf','cls','label']+[col for col in dfPred.columns if col[:3]=='pre']]\n",
    "      dfPred.to_csv(pathPred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
