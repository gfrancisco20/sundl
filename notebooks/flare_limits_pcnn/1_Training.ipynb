{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import * # notebooks config files -> retrieve path to sundl\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import dill as pickle\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if COLAB:\n",
    "  # mouting drive content in session on colab\n",
    "  mountDrive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sundl.utils.colab import mountDrive, ressourcesSetAndCheck, drive2local\n",
    "############################\n",
    "# SETUP\n",
    "############################\n",
    "\n",
    "# overwriting CLEAN_LOCAL :\n",
    "CLEAN_LOCAL = False\n",
    "\n",
    "if CLEAN_LOCAL:\n",
    "  shutil.rmtree(PATH_ROOT_LOCAL)\n",
    "  os.makedirs(PATH_ROOT_LOCAL)\n",
    "  \n",
    "# checking gpu and ram ressources\n",
    "ressourcesSetAndCheck(MIXED_PREC)\n",
    "\n",
    "############################\n",
    "# DATA IMPORT\n",
    "############################\n",
    "\n",
    "FILES2TRANSFER = {'images' : (PATH_ROOT_DRIVE_DS/'Images',        # source\n",
    "                              PATH_IMAGES,                        # dest\n",
    "                              ['eq_hmi_448', 'eq_193x211x94_448'] # files\n",
    "                              )\n",
    "                  }\n",
    "\n",
    "drive2local(FILES2TRANSFER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sundl.metrics import *\n",
    "\n",
    "\n",
    "\n",
    "labelCol     = 'mpf' # 'mpf' , 'toteh\n",
    "windowSizesH = [24]\n",
    "EPOCHS       = 1 # 25\n",
    "BATCH_SIZE   = 16\n",
    "IMG_SIZE     = (224, 448, 3) # (512, 1024, 3) (224, 448, 3)\n",
    "PTCH_SIZE    = (112, 112, 3) # (256, 256, 3) (112, 112, 3)\n",
    "\n",
    "NEW_FOLDER_NAME   = 'TEMP' #' \n",
    "CONTINUING_FOLDER = None #\n",
    "\n",
    "if labelCol=='mpf':\n",
    "  agg = 'max'\n",
    "else:\n",
    "  agg = 'sum'\n",
    "\n",
    "weightByClass = True\n",
    "\n",
    "CV_K      = 5 # None\n",
    "VAL_SPLIT = None # --> not used if CV_K not none\n",
    "\n",
    "SAMPLE_TRAIN = 0.05 # 0.95 #0.8 #0.7 #0.9\n",
    "SAMPLE_VAL = None\n",
    "\n",
    "CACHE     = True\n",
    "verbose   = 0\n",
    "\n",
    "SAVE_MODEL   = True\n",
    "save_monitor = 'val_tss3'\n",
    "save_mode    = 'max'\n",
    "save_thdS    = {'C': 0.50, \n",
    "                'M': 0.25, \n",
    "                'X': 0.10} \n",
    "\n",
    "RECOMPUTE_DATASET = True\n",
    "\n",
    "thresholds = [0.5]\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(threshold=0.5, name=f'acc')] \\\n",
    "        + [Tss(threshold=thd) for thd in thresholds] \\\n",
    "        + [Hss(threshold=thd) for thd in thresholds] \\\n",
    "        + [Mcc(threshold=thd) for thd in thresholds] \\\n",
    "        + [F1(threshold=thd) for thd in thresholds] \\\n",
    "        + [tf.keras.metrics.Precision(class_id = 1, name = 'precision')] \\\n",
    "        + [tf.keras.metrics.Recall(class_id = 1, name = 'recall')] \\\n",
    "        + [TP(threshold=thd) for thd in thresholds] \\\n",
    "        + [FN(threshold=thd) for thd in thresholds] \\\n",
    "        + [TN(threshold=thd) for thd in thresholds] \\\n",
    "        + [FP(threshold=thd) for thd in thresholds] \\\n",
    "        + [tf.keras.metrics.AUC(curve='ROC', name='auc_roc')] \\\n",
    "        + [tf.keras.metrics.AUC(curve='PR', name='auc_pr')]\n",
    "   \n",
    "# different weights and penalisation strategies  \n",
    "WEIGHT_BY_CLASS = True   \n",
    "weightCollection = {'EquiC'    : {'quiet': 0.25, 'B':0.25, 'C':0.167, 'M':0.167, 'X': 0.166},\n",
    "                    'EquiCnat' : {'quiet': 0.46, 'B':0.54, 'C':0.72, 'M':0.26, 'X': 0.03},\n",
    "                    'EquiM'    : {'quiet': 0.166, 'B':0.167, 'C':0.167, 'M':0.25, 'X': 0.25},\n",
    "                    'EquiMnat' : {'quiet': 0.28, 'B':0.32, 'C':0.40, 'M':0.91, 'X': 0.09},\n",
    "                    'ProgPos'  : {'quiet': 0.05, 'B':0.05, 'C':0.10, 'M':0.30, 'X': 0.50},\n",
    "                    'LowBC'    : {'quiet': 0.4, 'B':0.2, 'C':0.1, 'M':0.8, 'X': 0.8},\n",
    "                    'LowC'     : {'quiet': 0.4, 'B':0.4, 'C':0.1, 'M':0.8, 'X': 0.8},\n",
    "                    'LowC2'    : {'quiet': 0.2, 'B':0.2, 'C':0.1, 'M':0.8, 'X': 0.8} \n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.flare_limits_pcnn.utilsTraining import ModelInstantier2\n",
    "from sundl.models.blueprints import build_pretrained_PatchCNN\n",
    "from sundl.dataloader.sdocml import builDS_image_feature\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# Dataset common parameters\n",
    "ds_params = {'labelCol'    : labelCol,\n",
    "             'num_classes' : num_classes,\n",
    "             'img_size'    : IMG_SIZE\n",
    "             }\n",
    "\n",
    "# Models common parameters\n",
    "tfModel = tf.keras.applications.efficientnet_v2.EfficientNetV2S\n",
    "core_params = {'tfModel'        : tfModel,\n",
    "               'pretainedWeight': True,\n",
    "               'unfreeze_top_N' : 'all',\n",
    "               'num_classes'    : num_classes, # no use here\n",
    "               'img_size'       : IMG_SIZE,\n",
    "               'patches_size'   : PTCH_SIZE,\n",
    "               'regression'     : False,\n",
    "               'metrics'        : metrics,\n",
    "               'includeInterPatches' : False,\n",
    "               'loss' : BinaryCrossentropy(label_smoothing = 0,\n",
    "                                           name = 'loss'\n",
    "                                         )\n",
    "               } \n",
    "\n",
    "# Models definition\n",
    "# We just give as an example the fina EUV models of the paper\n",
    "PCNN_C =  ModelInstantier2(\n",
    "    buildModelFunction = build_pretrained_PatchCNN,\n",
    "    buildModelParams = dict(**core_params,\n",
    "                            **{'patche_output_type' : 'pre_pred',\n",
    "                               'meth_patche_agg'    : agg,\n",
    "                               'shared_patcher'     : 'all',\n",
    "                               'optimizer'          : AdamW(learning_rate = 1e-5,#  amsgrad = True,\n",
    "                                                            weight_decay  = 1e-4)\n",
    "                              }\n",
    "                            ),\n",
    "    buildDsFunction = builDS_image_feature,\n",
    "    buildDsParams =  ds_params,\n",
    "    name = f'C+_{labelCol}',\n",
    "    cls = 'C',\n",
    "    weightStrategy = 'ProgPos'\n",
    ")\n",
    "\n",
    "PCNN_M =  ModelInstantier2(\n",
    "    buildModelFunction = build_pretrained_PatchCNN,\n",
    "    buildModelParams = dict(**core_params,\n",
    "                            **{'patche_output_type' : 'pre_pred',\n",
    "                               'meth_patche_agg'    : agg,\n",
    "                               'shared_patcher'     : 'all',\n",
    "                               'optimizer'          : AdamW(learning_rate = 1e-5,#  amsgrad = True,\n",
    "                                                            weight_decay  = 1e-4)\n",
    "                              }\n",
    "                            ),\n",
    "    buildDsFunction = builDS_image_feature,\n",
    "    buildDsParams =  ds_params,\n",
    "    name = f'M+_{labelCol}',\n",
    "    cls = 'M',\n",
    "    weightStrategy = 'ProgPos'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.flare_limits_pcnn.utilsTraining import setUpResultFolder, conditionalHyperParameters, trainConstantModel, printTrainingResults, saveTrainingResults\n",
    "from sundl.utils.data import read_Dataframe_With_Dates, loadMinMaxDates\n",
    "from sundl.utils.flare.windows import windowHistoryFromFlList\n",
    "\n",
    "models = [(PCNN_C, ['0193x0211x0094'], h) for h in windowSizesH] + \\\n",
    "         [(PCNN_M, ['0193x0211x0094'], h) for h in windowSizesH] \n",
    "\n",
    "log, resDir, modelDir, mtcDict = setUpResultFolder(\n",
    "    models = models, \n",
    "    pathRes = PATH_RES,\n",
    "    metrics = metrics,\n",
    "    continuingFolder = CONTINUING_FOLDER, \n",
    "    newFolder = NEW_FOLDER_NAME,\n",
    "    imgSize = IMG_SIZE,\n",
    "    cv_K = CV_K,\n",
    "    saveModel = SAVE_MODEL\n",
    "    )\n",
    "\n",
    "print('\\nINITIAL STATUS : ')\n",
    "display(log)\n",
    "print('')\n",
    "res = {}\n",
    "best = None\n",
    "eval = None\n",
    "bestCVCrossEpoch = None\n",
    "dsTrain = None\n",
    "dsVal = None\n",
    "ct=0\n",
    "verbose = 1\n",
    "ct_dsBuilds = -1\n",
    "minDate, maxDate = loadMinMaxDates(PATH_IMAGES)\n",
    "flCatalog = read_Dataframe_With_Dates(PATH_FLCATALOG)\n",
    "print('minDate : ', minDate)\n",
    "print('maxDate : ', maxDate)\n",
    "\n",
    "for modelInstantiater, channels, h in tqdm(models):\n",
    "  ct_dsBuilds+=1\n",
    "  \n",
    "  dfFlareHistory = windowHistoryFromFlList(flCatalog, window_h = h, timeRes_h = 2, minDate = minDate, maxDate = maxDate)\n",
    "  \n",
    "  save_thd, labelCol, binCls, classWeights, classTresholds, encoder = conditionalHyperParameters(modelInstantiater, h, save_thdS, weightCollection)\n",
    "  modelInstantiater.buildDsParams['labelEncoder'] = encoder\n",
    "  modelInstantiater.buildDsParams['classTresholds'] = classTresholds\n",
    "  \n",
    "  CV_FLD_PTH  = F_PATH_FOLDS(labelCol, h)\n",
    "  with open(CV_FLD_PTH, 'rb') as f1:\n",
    "    dfFoldsTrainVal = pickle.load(f1)[0:CV_K]\n",
    "    \n",
    "  full_name_comb = modelInstantiater.fullNameFunc(channels,h)\n",
    "  if log.loc[full_name_comb]['status'] > 0:\n",
    "    print(f'\\n\\n-----------------------------\\n{full_name_comb} already successfuly trained\\n')\n",
    "  else:\n",
    "    log.loc[full_name_comb, 'status'] = -1\n",
    "    log.to_csv(resDir + '/log.csv')\n",
    "  \n",
    "    model = None\n",
    "    if RECOMPUTE_DATASET:\n",
    "      dsTrain = None\n",
    "      dsVal = None\n",
    "  \n",
    "    #===================================================\n",
    "    # CROSS VALIDATION LOOP\n",
    "    #===================================================\n",
    "    duration = time.time()\n",
    "    res[full_name_comb] = []\n",
    "    kf=0\n",
    "    for df_train, df_val in tqdm(dfFoldsTrainVal,disable = False):#not verbose):\n",
    "      print(f'\\n\\n-----------------------------\\nModel : {full_name_comb}')\n",
    "      if Path(resDir+f'/training_folds/training_{full_name_comb}_fd{kf:0>3d}.csv').exists():\n",
    "        res[full_name_comb].append(pd.read_csv(resDir+f'/training_folds/training_{full_name_comb}_fd{kf:0>3d}.csv').set_index('epoch'))\n",
    "        print(f'FOLD #{kf} ALREADY TRAINED')\n",
    "        kf += 1\n",
    "      else:\n",
    "        print(f'\\n\\n-----------------------------\\nModel : {full_name_comb}')\n",
    "        print(f'FOLD #{kf}')\n",
    "        \n",
    "        # MEMORY CLEANING\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        if model is not None: del model\n",
    "        if RECOMPUTE_DATASET:\n",
    "          if dsTrain is not None: del dsTrain\n",
    "          if dsVal is not None: del dsVal\n",
    "        gc.collect()\n",
    "        \n",
    "        # FOLDER MODEL\n",
    "        if SAVE_MODEL:\n",
    "          modelDirSub = modelDir + f'/{full_name_comb}'\n",
    "          if CV_K is not None:\n",
    "            modelDirSub = modelDirSub + f'_fd{kf:0>3d}'\n",
    "            \n",
    "        # DATASETS INSTANTIATION \n",
    "        dfSamples_train = df_train.copy()\n",
    "        dfSamples_val = df_val.copy()\n",
    "        if SAMPLE_TRAIN is not None:\n",
    "          dfSamples_train = dfSamples_train.sample(frac = SAMPLE_TRAIN, random_state=49)\n",
    "        if SAMPLE_VAL is not None:\n",
    "           dfSamples_val = dfSamples_val.sample(frac = SAMPLE_VAL, random_state=49)\n",
    "        if ct_dsBuilds==0 or RECOMPUTE_DATASET:\n",
    "          pathDir = PATH_IMAGES if channels is not None else None\n",
    "          dsTrain, _, missing_file_regexp, dfSamples_train_corr = modelInstantiater.build_DS(\n",
    "              pathDir    = pathDir,\n",
    "              channels   = channels,\n",
    "              dfTimeseries = dfFlareHistory.copy(), # dfFlareHistory\n",
    "              samples    = dfSamples_train.copy(), # dfSamples_train\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs     = EPOCHS,\n",
    "              cache      = CACHE,\n",
    "              shuffle    = True,\n",
    "              weightByClass = WEIGHT_BY_CLASS,\n",
    "              classWeights = classWeights,\n",
    "          )\n",
    "          print('')\n",
    "          dsVal, _, missing_file_regexp_val, dfSamples_val_corr  = modelInstantiater.build_DS(\n",
    "              pathDir    = pathDir,\n",
    "              channels   = channels,\n",
    "              dfTimeseries = dfFlareHistory.copy(),\n",
    "              samples    = dfSamples_val.copy(),\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs     = EPOCHS,\n",
    "              cache      = CACHE,\n",
    "              shuffle    = True,\n",
    "              weightByClass = False,\n",
    "              classWeights = None,\n",
    "              typeDs = 'val'\n",
    "          )\n",
    "          print(f'{len(missing_file_regexp)} incomplete training dates')\n",
    "          print(f'{len(missing_file_regexp_val)} incomplete val dates')\n",
    "          \n",
    "        # MODEL INSTANTIATION\n",
    "        model = modelInstantiater()\n",
    "        try:\n",
    "          print(f'\\nMODEL PARAMETERS #: {model.count_params()/1e6:.2f}M')\n",
    "          trainable_params = tf.reduce_sum([tf.reduce_prod(p.shape) for p in model.trainable_variables])\n",
    "          print(\"of which trainable #:\", trainable_params)\n",
    "        except:\n",
    "          pass\n",
    "        \n",
    "        # CALLBACKS\n",
    "        callbacks = []\n",
    "        if SAVE_MODEL:\n",
    "          callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n",
    "              modelDirSub,\n",
    "              save_best_only = True,\n",
    "              save_weights_only=False,\n",
    "              monitor = save_monitor,\n",
    "              verbose = 1,\n",
    "              mode = save_mode,\n",
    "              initial_value_threshold = save_thd)\n",
    "          )\n",
    "        \n",
    "        # TRAINING\n",
    "        if modelInstantiater.savedPredictionModel:\n",
    "          historyData = trainConstantModel(dsTrain, dsVal, model, modelInstantiater, EPOCHS, weightByClass, SAVE_MODEL, modelDirSub)\n",
    "        else:\n",
    "          history = model.fit(dsTrain,#.take(1),\n",
    "                              epochs=EPOCHS,\n",
    "                              validation_data = dsVal,#.take(1),\n",
    "                              callbacks = callbacks,\n",
    "                              verbose = 1 #verbose\n",
    "                              )\n",
    "          historyData = history.history\n",
    "          if SAVE_MODEL:\n",
    "            pathConfigModel = modelDirSub + f'_config.pkl'\n",
    "            modelInstantiater.saveConfig(pathConfigModel)\n",
    "        # vectorizing metric reesults\n",
    "        historyData = {m : np.array(historyData[m]) for m in historyData.keys()}\n",
    "        \n",
    "        # ADDITIONAL METRIC\n",
    "        historyData['far'] = 1 - historyData['precision']\n",
    "        historyData['val_far'] = 1 - historyData['val_precision']\n",
    "          \n",
    "        # PRINTING TRAINING HISTORY\n",
    "        printTrainingResults(historyData)\n",
    "\n",
    "      # SAVING FOLD RESULTS\n",
    "      res[full_name_comb].append(pd.DataFrame(historyData)) \n",
    "      res[full_name_comb][-1].index.names = ['epoch']\n",
    "      num_inst = len(dfSamples_train)\n",
    "      res[full_name_comb][-1]['num_train_inst'] = (res[full_name_comb][-1].index + 1) * num_inst\n",
    "      res[full_name_comb][-1].to_csv(resDir+f'/training_folds/training_{full_name_comb}_fd{kf:0>3d}.csv',index=True)\n",
    "       \n",
    "      kf+=1 # fold index\n",
    "      # END OF CV-LOOP\n",
    "      #===================================================\n",
    "      \n",
    "    # SAVING GENERAL RESULTS\n",
    "    res, best, bestCVCrossEpoch = saveTrainingResults(resDir, res, best, bestCVCrossEpoch, full_name_comb, CV_K)\n",
    "    \n",
    "    duration = time.time() - duration\n",
    "    log.loc[full_name_comb, 'status'] = 1\n",
    "    log.loc[full_name_comb, 'duration'] = f'{duration//3600:0>2.0f}h {duration//60%60:0>2.0f}m {duration%60:0>2.0f}s'\n",
    "    log.to_csv(resDir + '/log.csv')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = dsVal.skip(10).take(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ims = None\n",
    "labs = None\n",
    "for im, lab in ex:\n",
    "  if ims is None:\n",
    "    ims = im\n",
    "    labs = lab\n",
    "  else:\n",
    "    ims = tf.concat([ims,im],axis=0)\n",
    "    labs = tf.concat([labs,lab],axis=0)\n",
    "model.evaluate(ims,labs)\n",
    "labs = labs[:,1]\n",
    "pred = model(ims)[:,1]\n",
    "pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t=0.5\n",
    "tp = ((pred>=0.5) & (labs == 1)).numpy().sum()\n",
    "tn = ((pred<0.5) & (labs == 0)).numpy().sum()\n",
    "fp = ((pred>=0.5) & (labs == 0)).numpy().sum()\n",
    "fn = ((pred<0.5) & (labs == 1)).numpy().sum()\n",
    "\n",
    "tp, fn, tn, fp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = tp+fn\n",
    "n = tn+fp\n",
    "tss = tp / (tp+fn) + tn / (tn+fp) - 1\n",
    "hss = 2*(tp*tn - fn*fp) / (p*(fn+tn) + n*(tp+fp))\n",
    "mcc = (tp*tn - fp*fn) / np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "\n",
    "prec = tp / (tp+fp)\n",
    "far = 1-prec\n",
    "rec = tp / p\n",
    "f1 = 2*prec*rec / (prec+rec)\n",
    "\n",
    "tss, hss, mcc, f1, prec, rec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
