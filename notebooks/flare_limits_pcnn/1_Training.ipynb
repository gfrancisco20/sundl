{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training \n",
    "\n",
    "Notebooks to train models  \n",
    "  \n",
    "Examples is given for the EUV PCNNs of the paper    \n",
    "  \n",
    "Results are stored in PATH_RES/CONTINUING_FOLDER   \n",
    "(or PATH_RES/NEW_FOLDER_NAME when creating a new reslult folder)  \n",
    "\n",
    "The structure of a NEW_FOLDER_NAME is   \n",
    "automatically set up by the function utilsTraining.setUpResultFolder,   \n",
    "called here in the begining of the cell 'Training'  \n",
    "The content of those result folders is described in the doc config.py, below 'PATH_RES'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLAB = False\n",
    "\n",
    "if COLAB : \n",
    "  configSetup = {\n",
    "      'COLAB'           : 'True',\n",
    "      'PATH_ROOT_DRIVE' : '/content/drive/MyDrive/Projects/Forecast',\n",
    "      'PATH_ROOT_LOCAL' : '/content/session',\n",
    "      'PATH_SUNDL'      : '/content/sundl',\n",
    "      'PATH_PROJECT'    : '/content/sundl/notebooks/flare_limits_pcnn'\n",
    "  }\n",
    "  !git clone https://github.com/gfrancisco20/sundl.git\n",
    "  import sys\n",
    "  import re\n",
    "  sys.path.append(configSetup['PATH_SUNDL'])\n",
    "  sys.path.append(configSetup['PATH_PROJECT'])\n",
    "  configFile = f'{configSetup[\"PATH_PROJECT\"]}/config.py'\n",
    "  with open(configFile, 'r') as file:\n",
    "    content = file.read()\n",
    "  for constant in configSetup.keys():\n",
    "    content = re.sub(re.compile(f'{constant} = .*'), f'{constant} = \\'{configSetup[constant]}\\'', content)\n",
    "  with open(configFile, 'w') as file:\n",
    "    file.write(content)\n",
    "   \n",
    "from config import *\n",
    "from sundl.utils.colab import mountDrive\n",
    "if COLAB:\n",
    "  # mouting drive content in session on colab\n",
    "  mountDrive()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sunpy.net import Fido\n",
    "from sunpy.net import attrs as a\n",
    "event_type = \"FL\"\n",
    "tstart = \"2010/04/28\"\n",
    "tend = \"2023/04/29\"\n",
    "result = Fido.search(a.Time(tstart, tend),\n",
    "                     a.hek.EventType(event_type),\n",
    "                     a.hek.FL.GOESCls > \"C1.0\",\n",
    "                     a.hek.OBS.Observatory == \"GOES\")\n",
    "# Here we only show two columns due there being over 100 columns returned normally.\n",
    "print(result.show(\"hpc_bbox\", \"refs\"))\n",
    "\n",
    "# It\"s also possible to access the HEK results from the\n",
    "# `~sunpy.net.fido_factory.UnifiedResponse` by name.\n",
    "hek_results = result[\"hek\"]\n",
    "filtered_results = hek_results[\"event_starttime\", \"event_peaktime\",\n",
    "                               \"event_endtime\", \"fl_goescls\", \"ar_noaanum\"]\n",
    "\n",
    "filtered_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import dill as pickle\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sundl.utils.colab import mountDrive, ressourcesSetAndCheck, drive2local\n",
    "############################\n",
    "# SETUP\n",
    "############################\n",
    "\n",
    "# overwriting CLEAN_LOCAL :\n",
    "CLEAN_LOCAL = False\n",
    "\n",
    "if CLEAN_LOCAL:\n",
    "  shutil.rmtree(PATH_ROOT_LOCAL)\n",
    "  os.makedirs(PATH_ROOT_LOCAL)\n",
    "  \n",
    "# checking gpu and ram ressources\n",
    "ressourcesSetAndCheck(MIXED_PREC)\n",
    "\n",
    "############################\n",
    "# DATA IMPORT\n",
    "############################\n",
    "\n",
    "FILES2TRANSFER = {'images' : (PATH_ROOT_DRIVE_DS/'Images',        # source\n",
    "                              PATH_IMAGES,                        # dest\n",
    "                              ['eq_hmi_448', 'eq_193x211x94_448'] # files\n",
    "                              )\n",
    "                  }\n",
    "\n",
    "drive2local(FILES2TRANSFER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sundl.metrics.tfmetrics import *\n",
    "\n",
    "\n",
    "labelCol     = 'mpf' # 'mpf' -> windows's SXR-max-peak-flux , 'toteh' -> (hourly average of sum of flares' SXR-fluence)\n",
    "windowSizesH = [24]\n",
    "EPOCHS       = 1 # 25\n",
    "BATCH_SIZE   = 16\n",
    "IMG_SIZE     = (224, 448, 3) # (512, 1024, 3) (224, 448, 3)\n",
    "PTCH_SIZE    = (112, 112, 3) # (256, 256, 3) (112, 112, 3)\n",
    "\n",
    "NEW_FOLDER_NAME   = None                 # New folder in which to store results\n",
    "CONTINUING_FOLDER = 'Results_Paper_PCNN' # Existing foler in which to store results\n",
    "\n",
    "if labelCol=='mpf':\n",
    "  # aggregation type \n",
    "  agg = 'max'\n",
    "else:\n",
    "  agg = 'sum'\n",
    "\n",
    "weightByClass = True\n",
    "\n",
    "CV_K      = 5 \n",
    "VAL_SPLIT = None # --> not used if CV_K not none\n",
    "\n",
    "SAMPLE_TRAIN = None # 0.95 \n",
    "SAMPLE_VAL   = None\n",
    "\n",
    "CACHE     = True\n",
    "verbose   = 0\n",
    "\n",
    "SAVE_MODEL   = True\n",
    "save_monitor = 'val_tss3'\n",
    "save_mode    = 'max'\n",
    "save_thdS    = {'C': 0.50, \n",
    "                'M': 0.25, \n",
    "                'X': 0.10} \n",
    "\n",
    "RECOMPUTE_DATASET = True\n",
    "\n",
    "thresholds = [0.5]\n",
    "metrics = [tf.keras.metrics.BinaryAccuracy(threshold=0.5, name=f'acc')] \\\n",
    "        + [Tss(threshold=thd) for thd in thresholds] \\\n",
    "        + [Hss(threshold=thd) for thd in thresholds] \\\n",
    "        + [Mcc(threshold=thd) for thd in thresholds] \\\n",
    "        + [F1(threshold=thd) for thd in thresholds] \\\n",
    "        + [tf.keras.metrics.Precision(class_id = 1, name = 'precision')] \\\n",
    "        + [tf.keras.metrics.Recall(class_id = 1, name = 'recall')] \\\n",
    "        + [TP(threshold=thd) for thd in thresholds] \\\n",
    "        + [FN(threshold=thd) for thd in thresholds] \\\n",
    "        + [TN(threshold=thd) for thd in thresholds] \\\n",
    "        + [FP(threshold=thd) for thd in thresholds] \\\n",
    "        + [tf.keras.metrics.AUC(curve='ROC', name='auc_roc')] \\\n",
    "        + [tf.keras.metrics.AUC(curve='PR', name='auc_pr')]\n",
    "   \n",
    "# different weights and penalisation strategies  \n",
    "WEIGHT_BY_CLASS = True   \n",
    "weightCollection = {'EquiC'    : {'quiet': 0.25, 'B':0.25, 'C':0.167, 'M':0.167, 'X': 0.166},\n",
    "                    'EquiCnat' : {'quiet': 0.46, 'B':0.54, 'C':0.72, 'M':0.26, 'X': 0.03},\n",
    "                    'EquiM'    : {'quiet': 0.166, 'B':0.167, 'C':0.167, 'M':0.25, 'X': 0.25},\n",
    "                    'EquiMnat' : {'quiet': 0.28, 'B':0.32, 'C':0.40, 'M':0.91, 'X': 0.09},\n",
    "                    'ProgPos'  : {'quiet': 0.05, 'B':0.05, 'C':0.10, 'M':0.30, 'X': 0.50},\n",
    "                    'LowBC'    : {'quiet': 0.4, 'B':0.2, 'C':0.1, 'M':0.8, 'X': 0.8},\n",
    "                    'LowC'     : {'quiet': 0.4, 'B':0.4, 'C':0.1, 'M':0.8, 'X': 0.8},\n",
    "                    'LowC2'    : {'quiet': 0.2, 'B':0.2, 'C':0.1, 'M':0.8, 'X': 0.8} \n",
    "                    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.flare_limits_pcnn.utilsTraining import ModelInstantier2\n",
    "from sundl.models.blueprints import build_pretrained_PatchCNN\n",
    "from sundl.dataloader.sdocml import builDS_image_feature\n",
    "from tensorflow.keras.optimizers import Adam, AdamW\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "\n",
    "num_classes = 2\n",
    "\n",
    "# Dataset common parameters\n",
    "ds_params = {'labelCol'    : labelCol,\n",
    "             'num_classes' : num_classes,\n",
    "             'img_size'    : IMG_SIZE\n",
    "             }\n",
    "\n",
    "# Models common parameters\n",
    "tfModel = tf.keras.applications.efficientnet_v2.EfficientNetV2S\n",
    "core_params = {'tfModel'        : tfModel,\n",
    "               'pretainedWeight': True,\n",
    "               'unfreeze_top_N' : 'all', \n",
    "               'num_classes'    : num_classes, # no use here\n",
    "               'img_size'       : IMG_SIZE,\n",
    "               'patches_size'   : PTCH_SIZE,\n",
    "               'regression'     : False,\n",
    "               'metrics'        : metrics,\n",
    "               'includeInterPatches' : False,\n",
    "               'loss' : BinaryCrossentropy(label_smoothing = 0,\n",
    "                                           name = 'loss'\n",
    "                                         )\n",
    "               } \n",
    "\n",
    "# Models definition\n",
    "# We just give as an example the fina EUV models of the paper\n",
    "PCNN_C =  ModelInstantier2(\n",
    "    buildModelFunction = build_pretrained_PatchCNN,\n",
    "    buildModelParams = dict(**core_params,\n",
    "                            **{'patche_output_type' : 'pre_pred',\n",
    "                               'meth_patche_agg'    : agg,\n",
    "                               'shared_patcher'     : 'all',\n",
    "                               'optimizer'          : AdamW(learning_rate = 1e-5,#  amsgrad = True,\n",
    "                                                            weight_decay  = 1e-4)\n",
    "                              }\n",
    "                            ),\n",
    "    buildDsFunction = builDS_image_feature,\n",
    "    buildDsParams =  ds_params,\n",
    "    name = f'C+_{labelCol}',\n",
    "    cls = 'C',\n",
    "    weightStrategy = 'ProgPos'\n",
    ")\n",
    "\n",
    "PCNN_M =  ModelInstantier2(\n",
    "    buildModelFunction = build_pretrained_PatchCNN,\n",
    "    buildModelParams = dict(**core_params,\n",
    "                            **{'patche_output_type' : 'pre_pred',\n",
    "                               'meth_patche_agg'    : agg,\n",
    "                               'shared_patcher'     : 'all',\n",
    "                               'optimizer'          : AdamW(learning_rate = 1e-5,#  amsgrad = True,\n",
    "                                                            weight_decay  = 1e-4)\n",
    "                              }\n",
    "                            ),\n",
    "    buildDsFunction = builDS_image_feature,\n",
    "    buildDsParams =  ds_params,\n",
    "    name = f'M+_{labelCol}',\n",
    "    cls = 'M',\n",
    "    weightStrategy = 'ProgPos'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooks.flare_limits_pcnn.utilsTraining import setUpResultFolder, conditionalHyperParameters, trainConstantModel, printTrainingResults, saveTrainingResults\n",
    "from sundl.utils.data import read_Dataframe_With_Dates, loadMinMaxDates\n",
    "from sundl.utils.flare.windows import windowHistoryFromFlList\n",
    "\n",
    "models = [(PCNN_C, ['0193x0211x0094'], h) for h in windowSizesH] + \\\n",
    "         [(PCNN_M, ['0193x0211x0094'], h) for h in windowSizesH] \n",
    "\n",
    "log, resDir, modelDir, mtcDict = setUpResultFolder(\n",
    "    models = models, \n",
    "    pathRes = PATH_RES,\n",
    "    metrics = metrics,\n",
    "    continuingFolder = CONTINUING_FOLDER, \n",
    "    newFolder = NEW_FOLDER_NAME,\n",
    "    imgSize = IMG_SIZE,\n",
    "    cv_K = CV_K,\n",
    "    saveModel = SAVE_MODEL\n",
    "    )\n",
    "\n",
    "print('\\nINITIAL STATUS : ')\n",
    "display(log)\n",
    "print('')\n",
    "res = {}\n",
    "best = None\n",
    "eval = None\n",
    "bestCVCrossEpoch = None\n",
    "dsTrain = None\n",
    "dsVal = None\n",
    "ct=0\n",
    "verbose = 1\n",
    "ct_dsBuilds = -1\n",
    "minDate, maxDate = loadMinMaxDates(PATH_IMAGES)\n",
    "flCatalog = read_Dataframe_With_Dates(PATH_FLCATALOG)\n",
    "print('minDate : ', minDate)\n",
    "print('maxDate : ', maxDate)\n",
    "\n",
    "for modelInstantiater, channels, h in tqdm(models):\n",
    "  ct_dsBuilds+=1\n",
    "  \n",
    "  \n",
    "  save_thd, labelCol, binCls, classWeights, classTresholds, encoder = conditionalHyperParameters(modelInstantiater, h, save_thdS, weightCollection)\n",
    "  modelInstantiater.buildDsParams['labelEncoder'] = encoder\n",
    "  modelInstantiater.buildDsParams['classTresholds'] = classTresholds\n",
    "  \n",
    "  if F_PATH_WINDOWS('mpf', h).exists():\n",
    "    dfFlareHistory = read_Dataframe_With_Dates(F_PATH_WINDOWS('mpf', h))\n",
    "  else:\n",
    "    dfFlareHistory = windowHistoryFromFlList(flCatalog, window_h = h, timeRes_h = 2, minDate = minDate, maxDate = maxDate)\n",
    "  \n",
    "  \n",
    "  CV_FLD_PTH  = F_PATH_FOLDS(labelCol, h)\n",
    "  with open(CV_FLD_PTH, 'rb') as f1:\n",
    "    dfFoldsTrainVal = pickle.load(f1)[0:CV_K]\n",
    "    \n",
    "  full_name_comb = modelInstantiater.fullNameFunc(channels,h)\n",
    "  if log.loc[full_name_comb]['status'] > 0:\n",
    "    print(f'\\n\\n-----------------------------\\n{full_name_comb} already successfuly trained\\n')\n",
    "  else:\n",
    "    log.loc[full_name_comb, 'status'] = -1\n",
    "    log.to_csv(resDir + '/log.csv')\n",
    "  \n",
    "    model = None\n",
    "    if RECOMPUTE_DATASET:\n",
    "      dsTrain = None\n",
    "      dsVal = None\n",
    "  \n",
    "    #===================================================\n",
    "    # CROSS VALIDATION LOOP\n",
    "    #===================================================\n",
    "    duration = time.time()\n",
    "    res[full_name_comb] = []\n",
    "    kf=0\n",
    "    for df_train, df_val in tqdm(dfFoldsTrainVal,disable = False):#not verbose):\n",
    "      print(f'\\n\\n-----------------------------\\nModel : {full_name_comb}')\n",
    "      if Path(resDir+f'/training_folds/training_{full_name_comb}_fd{kf:0>3d}.csv').exists():\n",
    "        res[full_name_comb].append(pd.read_csv(resDir+f'/training_folds/training_{full_name_comb}_fd{kf:0>3d}.csv').set_index('epoch'))\n",
    "        print(f'FOLD #{kf} ALREADY TRAINED')\n",
    "        kf += 1\n",
    "      else:\n",
    "        print(f'\\n\\n-----------------------------\\nModel : {full_name_comb}')\n",
    "        print(f'FOLD #{kf}')\n",
    "        \n",
    "        # MEMORY CLEANING\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        if model is not None: del model\n",
    "        if RECOMPUTE_DATASET:\n",
    "          if dsTrain is not None: del dsTrain\n",
    "          if dsVal is not None: del dsVal\n",
    "        gc.collect()\n",
    "        \n",
    "        # FOLDER FOR MODEL\n",
    "        if SAVE_MODEL:\n",
    "          modelDirSub = modelDir + f'/{full_name_comb}'\n",
    "          if CV_K is not None:\n",
    "            modelDirSub = modelDirSub + f'_fd{kf:0>3d}'\n",
    "            \n",
    "        # DATASETS INSTANTIATION \n",
    "        dfSamples_train = df_train.copy()\n",
    "        dfSamples_val = df_val.copy()\n",
    "        if SAMPLE_TRAIN is not None:\n",
    "          dfSamples_train = dfSamples_train.sample(frac = SAMPLE_TRAIN, random_state=49)\n",
    "        if SAMPLE_VAL is not None:\n",
    "           dfSamples_val = dfSamples_val.sample(frac = SAMPLE_VAL, random_state=49)\n",
    "        if ct_dsBuilds==0 or RECOMPUTE_DATASET:\n",
    "          pathDir = PATH_IMAGES if channels is not None else None\n",
    "          dsTrain, _, missing_file_regexp, dfSamples_train_corr = modelInstantiater.build_DS(\n",
    "              pathDir    = pathDir,\n",
    "              channels   = channels,\n",
    "              dfTimeseries = dfFlareHistory.copy(), \n",
    "              samples    = dfSamples_train.copy(), \n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs     = EPOCHS,\n",
    "              cache      = CACHE,\n",
    "              shuffle    = True,\n",
    "              weightByClass = WEIGHT_BY_CLASS,\n",
    "              classWeights = classWeights,\n",
    "          )\n",
    "          print('')\n",
    "          dsVal, _, missing_file_regexp_val, dfSamples_val_corr  = modelInstantiater.build_DS(\n",
    "              pathDir    = pathDir,\n",
    "              channels   = channels,\n",
    "              dfTimeseries = dfFlareHistory.copy(),\n",
    "              samples    = dfSamples_val.copy(),\n",
    "              batch_size = BATCH_SIZE,\n",
    "              epochs     = EPOCHS,\n",
    "              cache      = CACHE,\n",
    "              shuffle    = True,\n",
    "              weightByClass = False,\n",
    "              classWeights = None,\n",
    "              typeDs = 'val'\n",
    "          )\n",
    "          print(f'{len(missing_file_regexp)} incomplete training dates')\n",
    "          print(f'{len(missing_file_regexp_val)} incomplete val dates')\n",
    "          \n",
    "        # MODEL INSTANTIATION\n",
    "        model = modelInstantiater()\n",
    "        try:\n",
    "          print(f'\\nMODEL PARAMETERS #: {model.count_params()/1e6:.2f}M')\n",
    "          trainable_params = tf.reduce_sum([tf.reduce_prod(p.shape) for p in model.trainable_variables])\n",
    "          print(\"of which trainable #:\", trainable_params)\n",
    "        except:\n",
    "          pass\n",
    "        \n",
    "        # CALLBACKS\n",
    "        callbacks = []\n",
    "        if SAVE_MODEL:\n",
    "          callbacks.append(tf.keras.callbacks.ModelCheckpoint(\n",
    "              modelDirSub,\n",
    "              save_best_only = True,\n",
    "              save_weights_only=False,\n",
    "              monitor = save_monitor,\n",
    "              verbose = 1,\n",
    "              mode = save_mode,\n",
    "              initial_value_threshold = save_thd)\n",
    "          )\n",
    "        \n",
    "        # TRAINING\n",
    "        if modelInstantiater.savedPredictionModel:\n",
    "          # models where input = output (e.g. persistant models)\n",
    "          historyData = trainConstantModel(dsTrain, dsVal, model, modelInstantiater, EPOCHS, weightByClass, SAVE_MODEL, modelDirSub)\n",
    "        else:\n",
    "          history = model.fit(dsTrain,#.take(1),\n",
    "                              epochs=EPOCHS,\n",
    "                              validation_data = dsVal,#.take(1),\n",
    "                              callbacks = callbacks,\n",
    "                              verbose = 1 #verbose\n",
    "                              )\n",
    "          historyData = history.history\n",
    "          if SAVE_MODEL:\n",
    "            pathConfigModel = modelDirSub + f'_config.pkl'\n",
    "            modelInstantiater.saveConfig(pathConfigModel)\n",
    "        # vectorizing metric reesults\n",
    "        historyData = {m : np.array(historyData[m]) for m in historyData.keys()}\n",
    "        \n",
    "        # ADDITIONAL METRIC\n",
    "        historyData['far'] = 1 - historyData['precision']\n",
    "        historyData['val_far'] = 1 - historyData['val_precision']\n",
    "          \n",
    "        # PRINTING TRAINING HISTORY\n",
    "        printTrainingResults(historyData)\n",
    "\n",
    "      # SAVING FOLD RESULTS\n",
    "      res[full_name_comb].append(pd.DataFrame(historyData)) \n",
    "      res[full_name_comb][-1].index.names = ['epoch']\n",
    "      num_inst = len(dfSamples_train)\n",
    "      res[full_name_comb][-1]['num_train_inst'] = (res[full_name_comb][-1].index + 1) * num_inst\n",
    "      res[full_name_comb][-1].to_csv(resDir+f'/training_folds/training_{full_name_comb}_fd{kf:0>3d}.csv',index=True)\n",
    "       \n",
    "      kf+=1 # fold index\n",
    "      # END OF CV-LOOP\n",
    "      #===================================================\n",
    "      \n",
    "    # SAVING GENERAL RESULTS\n",
    "    res, best, bestCVCrossEpoch = saveTrainingResults(resDir, res, best, bestCVCrossEpoch, full_name_comb, CV_K)\n",
    "    \n",
    "    duration = time.time() - duration\n",
    "    log.loc[full_name_comb, 'status'] = 1\n",
    "    log.loc[full_name_comb, 'duration'] = f'{duration//3600:0>2.0f}h {duration//60%60:0>2.0f}m {duration%60:0>2.0f}s'\n",
    "    log.to_csv(resDir + '/log.csv')\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
